---
title: "Species Distribution Models (SDM's)"
subtitle: "Analytical Toolbox for Ecologists and Evolutionary Biologists (PBC 470)"
author: "Reed Clark Benkendorf<br>Northwestern University & The Chicago Botanic Garden<br>Southwest Conservation Corps & Bureau of the Land Management"
date: "February 22, '23"
output:
  ioslides_presentation:
    widescreen: true
bibliography: ../citations/citations.bib
---

<style>

slide.title-slide h2 {
  text-align: left;
  color: #4E2A84;		
  line-height: 1.2;
}

slide.backdrop {
  background: black;
}

body {
  background-color: black;
}
h2 { 
 text-align: right;
 color: #4E2A84;		
}

h3 { 
 color: #3399ff;		
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
source('functions.R')
```


## talk overview

- Applications  
- Objectives
- Methods
- Interpretation

##  applications - conservation  

- aid in discovery of new populations of imperiled plants
- aid in creation of reserves under climate change models
- aid in predicting joint species distributions, i.e. obligate mutualisms 

## applications - academic

- fine scale models to study co-existence
- develop candidate species for metabarcoding

## objectives

using a number of known occurrences, identify areas in space which have
similar habitat which have the potential to support populations of the species of interest

but, what about dispersal?  
competition?  
mutualisms?  

## methods - overview

- define spatial domain and grain
- software environments
- dependent variables
- independent variables
- modelling approaches
- predicting a model into space

## domain and grain 

- domain; spatial extent of study
    - administrative boundary (e.g. State, Forest Preserve)
    - ecological model (e.g. **Omernik Ecoregions**)
- grain; scales in space and time
    - area at which process occurs (space)
    - **current and past climate** (time)
    - projected climates
    - (animals) seasonal patterns? 
- limitation: compute power

## software environments

- open source
    - **r**
      - **caret** *data partitioning* @kuhn2022caret
      - **sf** *vector data ala tidyverse* @pebesma2018sf
      - **terra** *raster data without headaches* @hijman2022terra
      - **sdm** *modelling operations*  @naimi2016sdm
    - **grass gis** *many modules for creating predictors*
    - **qgis** *graphical user interface for visualization*
    
## dependent variables - presences

- essential: occurrences of a species in space *(and time)*

for linear models, check for spatial autocorrelation of occurrence records using Moran's I; optional thin records stepwise, removing the most influential records.


```{r}
knitr::include_graphics('../graphics/BIEN.png')
knitr::include_graphics('../graphics/GBIF.png')
knitr::include_graphics('../graphics/FIA.png')
knitr::include_graphics('../graphics/AIM.png')
```


## dependent variables - absences

- optional: pseudoabsences of a species in space *(and time)*
- more optional: absences of a species in space *(and time)*

- if using presence and absence records
  - linear models, 1:*many* presence to absence points
  - machine learning, 1:1 presence to absence points

## independent variables

- variables relating to *patterns* in biotic distribution 
  - relevant to *your* extent and grains 
  - require *variation* 
  - no focus on factors governing biological *processes*, rather features which
  *correlate* with the known species distribution
- limitation: ompute power
  
## independent variables - examples

- domain: continental (e.g. North America)
  - maximum and minimum daily temperatures, precipitation amount and form, hydrologic drainage
- domain: regional (e.g. Southern Rockies)
  - elevation, soil classes, solar radiation
- domain: fine (e.g. East River Valley)
  - micro topography, water relations, shade, soils, species compositions
  
## variance in independent variables

explicitly check for variation to save compute resources

## modelling approaches - overview

- ensembles pt. I & II
- linear models
  - assumptions - dependent variables (IV)
  - assumptions - independent variables (DV)
  - modelling
    - model evaluation
  - machine learning
    - assumptions ?
    - modelling
      - model evaluation
- ensembles pt. III

## ensembles pt I

- problems with all models
  - garbage in -> garbage out
  - influential outliers
- with machine learning; 
  - models can fixate on these observations
  
- solution:
  - run many models, synthesize the results
  
  
> *"we are stronger together than we are alone"* - Walter Payton


<div class="notes">
These are some example notes in *markdown* format.

- List items can be included

- Like this
</div>

## ensembles pt II

- implementations:
  - weak learners; many simple decision tree models are combined to a single output
    - bootstrap aggregation (**bagging**) e.g. *random forests*
      - many models run independent of each  
    - **boosting** e.g. *boosted regression trees*
      - many models run sequentially, focusing on correcting errors in the last iteration
      
  - *stacking* another form of combining many models
    - can accommodate the consensus output from bagged, boosted, or traditional linear models
  
## linear models

- commonly implemented:
  - *generalized linear models* (glms)
  - *generalized additive models* (gams)

## linear models - assumptions DV

## linear models - assumptions IV

## modelling process

all evaluation performed by computer -- too much information

## machine learning 

much more common approach than individual linear models

species distributions are generally too complex for individual predictors, and building
fully interactive terms would take a long time.

the typical approach since the late 90's

do the work for you

## machine learning - assumptions

none, get a few observations, the more the merrier.

## modelling

train/test split (partition data)

- no free lunch 
  - no silver bullet machine learning algorithm; each is able to work better than 
  others under diverse circumstances.
  - try many types of models, select some that work for *your* application

- commonly implemented:
  - random forest 
  - maximum entropy (maxent)

## evaluation

- accuracy: proportion of all predictions which are 
  - sensitivity: probability of modelling giving a TRUE, when value is TRUE
  - specificity: probability of modelling giving a FALSE, when value is FALSE
- Area Under the ROC Curve (AUC)
- True Skill Statistic (TSS)
  - unequal sample sizes, i.e. evaluate many models at once
- Cohen's kappa 

## ensembles pt II

- several r packages offer *stacking* of many models, based on your selection 
of evaluation criteria to weigh them
- the 'test' partition of your data are used to evaluate this final model

## predicting a model into space 

- any model, based on values present in gridded (raster) data can be predicted onto a new raster surface
- each covariate in the model, is generally a single raster layer
- r packages, such as *terra*, do all the work for you
- accordingly, species distribution models create a map as a product

## interpretation

- this map has values of habitat suitability, based on your model. 

## tips and tricks

- keep a lab notebook; experiments *in silico* ala experiments *in vitro*

## conclusion

- species distribution models are very simple!
- fun introduction to simple machine learning
- represent a hypothesis of the probability of suitable habitat
- new avenues (J-SDM's) can include mutualisms
- stacked species (S-SDM's) distributions for predicting ecological assemblies

## contact info

- github/sagesteppe (preferred)
- reedbenkendorf2021@u.northwestern.edu


## some extra info 

## modelling ensemble learning

> __Ensemble learning__ utilizes many sets of trees, each tree being composed of many binary decisions, to create a single model. Each independent variable ( - or *feature*) may become a node on the tree - i.e. a location on the tree where a binary decision will move towards a predicted outcome. Each of the decision tree models which ensemble learning utilizes is a weak model, each of which may suffer due to high variance or bias, but which produce better outcomes than would be expected via chance. When ensembled these models generate a strong model, a model which should have more appropriately balanced variance and bias and predicts outcomes which are more strongly correlated with the expected values than the individual weak models. 

## modelling random forest

> *__Random Forest (RF)__* the training data are continually bootstrap re-sampled, in combination with random subsets of features, to create nodes which attempt to optimally predict a known outcome. A large number of trees are then aggregated, via the most common predictions, to generate a final classification prediction tree. Each individual prediction tree is generated independently of the others. 

## modelling boosted regression trees

> *__Boosted Regression Tree (BRT)__* (or Gradient Boosted tree) An initial tree is grown, and all other trees are derived sequentially from it, as each new tree is grown the errors in responses from the last tree are weighed more heavily so that the model focuses on selecting dependent variables which refine predictions. All response data and predictor variables are kept available to all trees.

## citations
