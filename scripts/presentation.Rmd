---
title: "Species Distribution Models (SDM's)"
subtitle: "Analytical Toolbox for Ecologists & Evolutionary Biologists (PBC 470)"
author: "Reed Clark Benkendorf<br>Northwestern University & The Chicago Botanic Garden<br>Southwest Conservation Corps & Bureau of the Land Management"
date: "February 22, '23"
output:
  ioslides_presentation:
    widescreen: true
    logo: ../graphics/cbg_logo.jpg
bibliography: ../citations/citations.bib
---

<style>

slide.title-slide h1 {
  font-size: 2.0em;
  line-height: 1.5;
}

slide.title-slide h2 {
  font-size: 1.0em;
  text-align: left;
  color: #4E2A84;		
  line-height: 1.2;
}

slide.backdrop {
  background: black;
}

body {
  background-color: black;
}

h2 { 
 text-align: right;
 color: #4E2A84;		
}

h3 { 
 color: #3399ff;		
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
source('functions.R')
```


## talk overview

- Applications  

- Objectives

- Methods

- Interpretation

##  applications - conservation  

- aid in discovery of new populations of imperiled plants

- aid in creation of reserves under climate change models

- aid in predicting joint species distributions, i.e. obligate mutualisms 

## applications - academic

- fine scale models to study co-existence
- develop candidate species for metabarcoding

## objectives {.build}

using known occurrences of a species, identify areas which have
similar habitat and the potential to support populations 

but, what about dispersal?  

competition?  

mutualisms?  

## an example species to accompany our study

 [*Besseya* (=*Synthris*) *alpina* (A. Gray) Rydberg.](http://floranorthamerica.org/Synthyris_alpina)
 
<div class="columns-2">


![American Basin](../graphics/American_Basin.jpg){width=75% height=75%}

![*B. alpina*, Franklin #3948](../graphics/Besseya_alpina_sheet.jpg){width=75% height=75%}
</div>

<div class="notes">

As an example species, we are going to use one that i modelled as a portion of masters work. *Besseya alpina*, alpine kittentails, is a part of the Plantaginaceae and closely aligned with *Veronica*. This species is endemic to Western North America, and it is confined entirely to rocky or stony fellfields in the alpine. as would be seen 

</div>

## methods - overview {.build}

define spatial domain and grain

software environments

dependent variables

independent variables

modelling approaches

model evaluation

predicting a model into space

<div class="notes">

the general steps for creating species distribution models are generally pretty similar as for most other projects. One of the real first pieces of information to work towards figuring out is how much computer power you have, and that will inform the domain and grains. Generally who you are working with, or what experience with software you have wil determine the environments you are using. Acquiring species records, and the independent variables can take some review time, but both are now largely available at specialized databasea for them. The choices of modelling approaches to attempt again come largely down to personal experience and familiarity with a type of model or algorithms. Evaluation kind of comes down to where you would feel the least bad about being wrong; do you want to over or under predict? What are the consequences of preding a false presence? 

</div>

## domain and grain {.build .flexbox .vcenter}

domain; spatial extent of study  
  - administrative boundary   
  - ecological model 
    
grain; scales in space and time  
- resolution at which process occurs (space)    
- current and past climate (time)    
- projected climates    
- (animals) seasonal patterns?     

limitation: compute power  

<div class="notes">

The first few steps in your analysis is to reasonably define a few parameters. It is easy to get carried away when you do not have to collect field data yourself, and try to perform analyses at enormous spatial scales, or at very very fine details.

Remember the 'domain' is the spatial extent over which we are interested in studying a process. Generally, these are defined by funding agencies, e.g. for all of my normal office work, I use extents of land ownership; a form of administrative boundary. For academic work, depending on how the funding comes through, we may be much more likely to use domains informed by ecological processes. Examples would be a floristic province, or an ecoregion concept. 

Grain is the resolution of an event in both time and space. For a spatial grain, you want to gather predictor variables which are able to correlate well with your species. For example day length in spring for plants, will only vary substantially over quite large scales. Elevation at a scale of 1 meter, rather than a more coarse surface would contain much meaningless variation. So we must balance these; however, most publicly available spatial data come only at a few resolutions (800m^2^, 1000m^2^, or 4000m^2^).

Temporal grain is very important for migratory animals, it is also very important if you want to see a species distribution before climate change effects, and after, and if you want to project your results into the future under climate change projections. 

It sounds like you will have many options to sort through here, but no! You are informed by computer power, you do what you can, just like with fieldwork. 

</div>

## domain and grain {.flexbox .vcenter}

![Domain](../graphics/domain.resized.png){height=100%}

<div class="notes">

Our species Besseya alpina is endemic to the Southern Rocky Mountains. During this modelling endeavor we will do what most Species distribution modellers do and model the whole range. As we see here, we have a focal area, in greenish blue, which constitutes an Ecological Region, an area with a clustered set or properties. We then buffer this area by a moderate amount, around 100 kilometers, and we have established a domain. 

</div>


## software environments
<div class="columns-2">
- **R**
  - **sf** *vector data ala tidyverse* @pebesma2018sf
  - **terra** *raster data without headaches* @hijman2022terra
  - **sdm** *modelling operations*  @naimi2016sdm
  - **caret** *ML / data partitioning* @kuhn2022caret
- **grass gis** *many modules for creating predictors*
- **qgis** *graphical user interface for mouse guided visualization*

![](../graphics/terra.resized.png)
![](../graphics/sf.resized.png)
![](../graphics/caret.resized.png)
![](../graphics/grass.resized.png)
![](../graphics/qgis.resized.png){height=50%, width=50%}

</div>

<div class="notes">

There are a handful of options which exist for generating species distribution models and processing geographic information systems data. I can only advocate the use of open source software, as this allows others to benefit and reproduce our research without considerable fiscals resources. Furthermore, open source alternatives are generally much better than paid software. As species distribution models are really all about the statistics, R, is an un- paralled software to generate SDM's in, and will keep your notes for you! However, grass gis offers several modules, all of which are accessible from the command line, which create useful spatial data products. Likewise QGIS is a really great lightweight GIS with a super friendly graphical user interface which you can use to visualize your predictor variables more dynamically than the experience offered by R and Rstudio. 

We rely heavily on the sdm package, which is one of the best examples of *object orientated programming* which I have seen in R, and is capable of loading in statistical methods from nearly any R package you want! For example if you want any flavor of linear models you can import new methods from any Ben Bolker package you want! 

</div>
    
## dependent variables - presences

<div class="columns-2">
- occurrences of a species in space *(and time)*
- geographic accuracy / spatial grain
- Linear models:
  - check for spatial autocorrelation using Morans I
  - sampling artifact? remove samples
  - thin points stepwise by most 'offending' record


![](../graphics/BIEN.resized.png)
![](../graphics/GBIF.resized.png)
![](../graphics/superbloom_17.resized.jpg){height=50%}

</div>

<div class="notes">

SO you need records of plant occurrences. The unequivocally most useful, and unlikely to ever be approached, source for these are herbaria. These things are huge people, the Field Museum Herbarium alone has 2.7 million records; these are enormous datasets we have at our disposal. Fortunately, what started to be developed when I came on the scene are what we call digital herbaria consortia, which collect records from all herbaria in a region in one place. What started to exist when you all came around were the 'aggregators' which aggregated the records from the consortia, and now citizen science records! These make getting records absurdly easy, and I kid you not, I hear tales of the invention of a 'conglomerator' for the aggregators on the breezes that blow from the east. 

</div>

## dependent variables - presences  example {.flexbox .vcenter}

![Occurence Records](../graphics/besseya_occ.png){height=100%}

<div class="notes">

In general, when it comes to rare species your only sources of occurrence records are going to be from herbaria, and subsequent targeted sampling for that species, or surveying for  an entirely rare habitat type which this and other species occur in. Usually, for these records you want to work with a Natural Heritage Program, or Natural History Survey; they tend to specialize in these surveys and maintenance of these records.  One problem with old records, which is somewhat ameliorated for rare species, is the accuracy of the records geolocation. Old maps were not exceptional, and most botanists don't press or note exactly where a specimen came from in the old days. 

For more common species, you are able to now glean considerable amounts of information from citizen science resources; although these tend to be phenotypically biased towards pretty flowers, at the expense of truly gorgeous flowers such as those in groups like the Sedges. 

</div>

## dependent variables - absences

- optional: pseudoabsences space *(and time)*
- more optional: absences space *(and time)*

<div class="columns-2">

- presence:absence records  
  - linear models - 1:*many*  
  - machine learning - 1:1  
- distance between presences
  - geographic
  - environmental
  
  ![correlated!](../graphics/AIM.resized.png)

</div>
  
```{r}
knitr::include_graphics('../graphics/FIA.png')
```


## dependent variables - absences example {.flexbox .vcenter}

**just create some random red points and add em to the map above.** 

## independent variables

- variables relating to *patterns* in biotic distribution 
  - relevant to *your* extent and grains 
  - require *variation* 
  - no focus on factors governing biological *processes*, rather features which
  *correlate* with the known species distribution
- limitation: compute power

<div class="notes">

OK, so here is an essential component of SDM's, they are NOT causative! They rely entirely on CORRELATION, even if you are using variables which you know have causal relationships with plant distributions the models that we are creating will not really reflect causal processes. What we want to gather together, are variables which we think co-occur with the species! Generally these pretty much end of being causal variables, but we do not sell them as such. 

These variables need to reflect processes which vary in both your domain, and within your grains. 

</div>
  
## independent variables - examples {.build .flexbox .vcenter}

domain: continental (e.g. North America)    
    - maximum and minimum daily temperatures [monthly, 4km]  
    - precipitation  [monthly, 4km]  
    - hydrologic drainage [millenial, 4km]  
  
domain: regional (e.g. Southern Rockies)    
    - elevation  [millenial, 1km]  
    - soil classes  [millenial, 1km]  
    - solar radiation [millenial, 1km]  
    - precipitation form [monthly, 1km]  
  
domain: fine (e.g. McDonald Woods)    
    - micro topography   [decade, 1m]  
    - water relations  [decade, 1m]  
    - shade  [weekly, 1m]  
    - soils  [decade, 1m]  
    
<div class="notes">

So as an example of how domain may affect our selection of independent variables, we can image we are interested in modelling species at three different levels. The first is across North America, we will probably have very good results using rather simple variables, at relatively coarse grains. For example, the maximum temperature - e.g. heat stress, and minimum temperatures are linked to the movement of moisture in plants and have enormous effects on distributions. Even the two sides of the Mississippi River Drainage basin, largely this an area so flat for nearly a thousand miles you cannot even detect changes in elevation or slope; might have the single largest effect on the flora of north america!

When we move to less extensive domains our predictors will change. We may start to gather very good information from toporgraphic relief, especially in the west; soil now be at a scale where it correlates with distributions at the class level, and solar radiation may start to tease out mesic north facing forested areas. 

As we move to fine domains, and these really are the frontier for these models, especially in academic settings, we are really starting to get to very directly related features. Here we can really start to integrate field experiments, and nitty gritty natural history observations with species distribution models. Assuming you can find the spatial data to do so!

</div>

    
## independent variables - specific examples

<div class="columns-2">

- Percent bedrock *(rocky, young soils)*
- Elevation *(alpine habitat)*
- Bare ground *(few others plants?)*
- X-Y coords *(alpine zone decreases with latitude)*
- Soil surface pH *(calcareous bedrock?)*
- Precipitation as snow *(monsoonal influence?)*

  
![](../graphics/alpine_plant_life.resized.jpg)
</div>

## variance in independent variables

<div class="columns-2">

  - explicitly check for variation
  
  - carefully encode categorical data
  
  - too much, may not be useful
  
  - too little, may not be useful
  
  - pilot knock out studies; use one variable leaving the others out
  
  - warrants simplifying a variable?
  
  - t.test the difference in values between presence and absence points
  
  ![](../graphics/variation.resized.png)
</div>

<div class="notes">

OK so more of what we have discussed already. You are compute limited, and can't be dragging along free loading variables. If you are including a variable, be sure that it is going to contribute something within the domain and at the scales which you are working with. 

People get pretty into this, especially with all types of machine learning - which we'll discuss later, but basically try and reduce your predictors down to those which offer something. The simplest approach is just viewing the variance and seeing if you have anything low, for example that top-most bar.  The next level is comparing the values which are obtained from your presence and absence points with a t.test, no difference? why keep it. A more laborious, but more useful approach, involve creating simple models, and seeing which variables are meaningful predictors of the response using selection procedures. 

</div>

## modelling approaches - overview

- ensembles pt. I & II
- linear models
  - assumptions - dependent variables (IV)
  - assumptions - independent variables (DV)
  - modelling
    - model evaluation
  - machine learning
    - assumptions ?
    - modelling
      - model evaluation
- ensembles pt. III

## ensembles pt I {.build}

problems with all models  
  - garbage.(in) -> garbage.(out)    
  - influential outliers  

with machine learning;  
  - models can fixate on these observations      
  
solution:  
  - run many models, synthesize the results    
  
<br>
<br>
<div class="centered">
> *"we are stronger together than we are alone"* - Walter Payton
</div>

## ensembles pt II  {.build }

weak learners; many simple decision tree models are combined to a single output

bootstrap aggregation (**bagging**) e.g. *random forests*    
  - many models run independent of each    
    
**boosting** e.g. *boosted regression trees*    
  - many models run sequentially, focusing on correcting errors in the last iteration
      
*stacking*   
   - can accommodate the consensus output from bagged, boosted, or traditional linear models
  
## linear models

- commonly implemented:
  - *generalized linear models* (glms)
  - *generalized additive models* (gams)

## linear models - assumptions DV

- distinct records, e.g. no duplicates of herbaria specimens    
- one record per cell of gridded surface    

## linear models - assumptions IV

<div class="columns-2">
  - Variance inflation (vifstep or vifcor)
  - identify correlated variables
  - pilot knock out studies; use one variable of the set as a predictor leaving the others out
  
  ![correlated!](../graphics/vifstep.resized.png)
</div>

<div class="notes">

So, I'm sure as you all have seen environmental predictors are generally highly correlated. If you are going to be using them for species distribution models which you will be, check them. There are not special considerations for spatial contexts worth worrying about. 

</div>

## modelling process {.flexbox .vcenter}

all evaluation performed by computer -- too much information

## machine learning 

much more common approach than individual linear models

many 'weak learners'

species distributions are generally too complex for individual predictors, and building
fully interactive terms would take a long time.

the typical approach since the late 90's

do the work for you

## machine learning - assumptions {.flexbox .vcenter}

none, get a few observations, the more the merrier.

## modelling

train/test split (partition data)

- no free lunch 
  - no silver bullet machine learning algorithm  
  - each is able to work better than others under diverse circumstances  
- try many types of models, select some that work for *your* application

- common algorithms:
  - maximum entropy (maxent)
  - random forest (rf)
  - boosting (brt)
  - support vector model (svm)

## evaluation I {.build}

Practitioners are *always* wrong. 

How do you *want* to be wrong?   

the downsides of predicting suitable habitat where it isn't?

the downsides of predicting non-suitable habitat where it really is?    

What is the cost of *'better'* predictions?  

other projects? priorities? deadlines?   

<div class="notes">

If you are in this room right now, it is because you are more familiar with being right than being wrong. Which is excellent in most ways, except it has probably created an aversion to being wrong for you, which can lead to an aversion to taking risks; but we'll side line that. 

*Practitioners are always wrong. *
But folks, life is not a written test. It has problems you, or really rather your teachers, will not let you ignore. and I gurantee you every day a botanists has to make more than a few decisions one will be wrong; and somehow alot of these will end up right in the end too by some bizarre miracle. Enough sentiment!

*how do you want to be wrong*
Just like a controlled fall, when we start making models, if we hope to apply them, we want to understand how to fall with the least harm. So which parts of your models and predictions are you willing to sacrifice to being less correct than the others?

*the downsides of predicting suitable habitat where it isn't?*
If you are working on restoration projects and you are working to decide what to plant, you may seek advice from a climate change projection seeding map. If this map is wrong, you are likely to be out hundreds of thousands of dollars in error, at the expense of purchasing other seeds with this money. 

*the downsides of predicting non-suitable habitat where it really is?*
If you are trying to create a very large reserve, by looking at a variety of overlapping species distributions, and you have agreeable partners, than the consequences are very minimal. 

*What is the cost of better predictions?*
You may find like you have no time to do the tihngs you want to do being in Winter quarter and all, but things will slow down some time soon. Things will slow down, but you never have the time to do what you wan tto do, and you are going to need to prioritize! What are the consequences of hyper tuning and parameratizing you model? 

</div>


## evaluation II {.build}

$$ Accuracy = \frac{\text{correct classifications}}{\text{all classifications }} $$

$$ Sensitivity = \frac{\text{true positives}}{\text{true positives + false negatives }} $$ probability of the method giving a positive result when the test subject is positive.


$$ Specificity = \frac{\text{true negatives}}{\text{true negatives + false positives }} $$ probability of the method giving a negative result when the test subject is negative  


## evaluation III

- Area Under the ROC Curve (AUC)  
- True Skill Statistic (TSS)  
  - unequal sample sizes, i.e. evaluate many models at once  
- Cohen's kappa

## ensembles pt II

- several r packages offer *stacking* of many models, based on your selection 
of evaluation criteria to weigh them
- the 'test' partition of your data are used to evaluate this final model

## predicting a model into space 

- any model, based on values present in gridded (raster) data can be predicted onto a new raster surface
- each covariate in the model, is generally a single raster layer
- r packages, such as *terra*, do all the work for you
- accordingly, species distribution models create a map as a product

## interpretation 

- percent suitability of habitat
- of each cell - not proportion of cell
- based on your model
  

## interpretation - example {.flexbox .vcenter}

![Occurence Records](../graphics/besseya_predicted.resized.png){height=100%}

## tips and tricks

- keep a lab notebook; this is bench science 

- always start models small (avoid computer crashes)  

- use strong and discrete directory organization  

- scratch paper, whiteboards, flowcharts

- dynamic programming; import/export data

- track code on github  

## objectives - recapped {.build}

using known occurrences of a species, identify areas which have
similar habitat and the potential to support populations 

mutualisms?  

*modelling two species together, joint-SDMs*

competition?  

*using suitability surfaces to plant assemblages of species in field experiments'*

dispersal?  

*using gridded surfaces to model the probability of dispersal from known to suitable habitat*  

## conclusion

- species distribution models are very simple!
- fun introduction to simple machine learning
- represent a hypothesis of the probability of suitable habitat
- new avenues (J-SDM's) can include mutualisms
- stacked species (S-SDM's) distributions for predicting ecological assemblies

## contact info {data-background=../graphics/collecting.jpg size=cover, .flexbox .vcenter}

<div class="white"; 
font-size: 3.0em; >
- github/sagesteppe
</div>

## some extra info 

## modelling resources


two hour discussion of the ['sdm'](https://www.youtube.com/watch?v=83dMS3bcjJM) package by an author

large [repository](https://github.com/sagesteppe/SDMS_RMBL) for high throughput modelling 

large [repository](https://github.com/sagesteppe/Spatial_Data_Science_R) about spatial data in R

short [activity](https://github.com/sagesteppe/Spatial_Data_Science_R/blob/main/SDS_Laboratory.R) using a sdm like process to teach spatial data

## modelling ensemble learning {.flexbox .vcenter}

> __Ensemble learning__ utilizes many sets of trees, each tree being composed of many binary decisions, to create a single model. Each independent variable ( - or *feature*) may become a node on the tree - i.e. a location on the tree where a binary decision will move towards a predicted outcome. Each of the decision tree models which ensemble learning utilizes is a weak model, each of which may suffer due to high variance or bias, but which produce better outcomes than would be expected via chance. When ensembled these models generate a strong model, a model which should have more appropriately balanced variance and bias and predicts outcomes which are more strongly correlated with the expected values than the individual weak models. 

## modelling random forest {.flexbox .vcenter}

> *__Random Forest (RF)__* the training data are continually bootstrap re-sampled, in combination with random subsets of features, to create nodes which attempt to optimally predict a known outcome. A large number of trees are then aggregated, via the most common predictions, to generate a final classification prediction tree. Each individual prediction tree is generated independently of the others. 

## modelling boosted regression trees {.flexbox .vcenter}

> *__Boosted Regression Tree (BRT)__* (or Gradient Boosted tree) An initial tree is grown, and all other trees are derived sequentially from it, as each new tree is grown the errors in responses from the last tree are weighed more heavily so that the model focuses on selecting dependent variables which refine predictions. All response data and predictor variables are kept available to all trees.

## citations
